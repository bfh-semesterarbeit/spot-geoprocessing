\section{Realisierung des Prototyps}
Als Umgebung, um die Infrastruktur für den Prototypen bereitzustellen, hat der Autor einen neuen AWS Account eröffnet, um kostenlose Kontingente an Services und Produkten nutzen zu können \cite{FreeTier2020}.

Nachdem der erwähnte AWS Account erstellt war, mussten erste Schritte, wie grundlegende Konfigurationen des Identity Access Management (IAM) gemacht und das Steuern von Spot Anfragen, das Einrichten eines Container Repositories (ECR), das Aufsetzen eines öffentlich zugänglichen S3 Buckets und anderes aus dem AWS Produktekatalog, erlernt werden.

Weitere Schritte folgten. Die eigentliche Realisierung des Prototypen wird in den folgenden Kapiteln beschrieben werden. Das Endresultat wird jedoch der Nachvollziehbarkeit halber schon einmal hier abgebildet:

\begin{figure}[H]
	\centering
	\includegraphics[width=.90\textwidth]{poc_zustand}
	\caption{Geodatenverarbeitung mit SPOT Instanzen.}
	\label{fig:ist_zustand}
\end{figure}

\subsection{Spot Flottenanfrage}
Als Setup für die Spot Flottenanfrage wurde ein sogenanntes Launch Template mit den wichtigsten Konfigurationen wie Sicherheitsgruppe, Wahl des AMI und User Data angelegt. Anschliessend wurde eine Spot Flottenanfrage, basierend auf diesem Template erstellt. Wobei als Mindestanforderung an den Rechner 16 CPUs und 60 GB Memory waren. Als Option wurde gewählt, dass die Zielkapazität aufrechterhalten bleiben soll: Somit wird nach jedem Interrupt automatisch eine neue Instanz mit der definierten Konfiguration zur Verfügung gestellt.

\subsection{Handling der Interrupts}
Wie im Kapitel \ref{kap:bugdet_instanzen} beschrieben, sind Spot Instanzen so günstig, weil sie einem jederzeit weggenommen werden könnten. Also muss die Datenverarbeitung mit dem Handling von Interrupts umgehen können. 
Der Einfachheit halber hat der Autor gänzlich auf das \textit{Horchen eines bevorstehenden Interrupts} via RESTful Abfrage (In Anhang \ref{appendix:restful} beschrieben) oder \emph{CloudWatch} verzichtet und hat stattdessen die Datenverarbeitung als Serie betrachtet und in Schritte unterteilt. Sobald ein Schritt erledigt wird, wird dies auf dem EFS Volumen in einer Textdatei festgehalten. Falls es zu einem Interrupt kommen sollte, wird von der Spot Flotte die nächste Instanz bereitgestellt und die Publizierung macht bei dem Schritt weiter, der zuletzt in der Textdatei festgehalten wurde (Abbildung \ref{fig:ist_zustand}, Nr. 5).

\subsection{Datenverarbeitung als Code}
Mit der Entscheidung, die Spot Instanz via \emph{Cloud-Init}-Ansatz (Abb. \ref{fig:ist_zustand}, Nr. 1) mit Ansible zu Provisionieren\footnote{Und nicht mit einem neuen AMI.}, lag es auf der Hand, auch gleich dieselbe Technologie für die Steuerung der gesamten Datenverarbeitung zu verwenden (Nr. 3). 
Viele Annehmlichkeiten, wie die Nähe zur Shell und Python, das Logging und der deklarative Code zeigten sich in der Realisierung als hilfreich. Eines der ersten Kommandos, welches das Initialisierungsskript ausführt, ist das Klonen des Codes von github.com (Abb. \ref{fig:ist_zustand}, Nr. 2). Das Init-Skript wie auch das Ansible Playbook können auf \href{https://github.com/bfh-semesterarbeit/up-and-running-dataprocessing}{github.com/bfh-semesterarbeit} eingesehen werden.


\subsection{Testen der Datenstruktur}
Wie in Kapitel \ref{kap:sicherstellung_qualitaet} erwähnt, ist es auch schon vorgekommen, dass die gelieferten Rohdaten korrupt waren. Da alle gelieferten Rohdaten (KML und Collada) in XML geschrieben sind, wurde beschlossen, einen kleinen Test mit grosser Wirkung in den automatischen Publikationsprozess mit einzubeziehen. Dieser Test macht nichts anderes, als die Dateien auf well-formed XML zu testen. 
Dieser Test schreibt den Namen aller fehlerhaften XML Dateien in eine Liste. Nach dem Testen wird die Länge dieser Liste überprüft und falls diese nicht leer ist, wird der gesamte Datenverarbeitung gestoppt. Diese Liste kann dann dem Auftraggeber zur Nachbearbeitung der fehlerhaften Daten ausgehändigt werden. Dieser Schritt passiert innerhalb Schritt Nr. 3 der  Abbildung \ref{fig:ist_zustand}.

\subsection{Datenverarbeitung}
Die eigentliche Datenverarbeitung, der Formatumbau der Rohdaten in das Web Format Cesium3DTiles, wird mittels Docker Container ausgeführt\footnote{In dieser Arbeit manchmal als AGI-Container bezeichnet.}. Um nicht von externen Services wie dockerhub.io abhängig zu sein und aus Neugier, wurde dafür ein AWS Container Repository\footnote{ECR} eingerichtet (Abbildung \ref{fig:ist_zustand}, Nr. 4.) \cite{9781484251003}. Das Kopieren der fertig verarbeiteten Webdaten auf S3 wird über die AWS CLI gemacht (Abb. \ref{fig:ist_zustand}, Nr. 6).

\subsection{Monitoring}
Bisher wird die Datenverarbeitung lediglich geloggt. Die verwendeten Tools\footnote{Der well-formed XML Test und der Output des Containers.} wie auch Ansible loggen in ein Verzeichnis im EFS, das gut zugänglich ist. Vor allem die Logdatei von Ansible ist ausführlich und macht eine Fehlersuche einfach.

Um die Last des Servers zu überprüfen, wurde lediglich die Standardüberwachung der Amazon Webconsole verwendet.

Bei einer allfälligen Integration des Prototypen in den Betrieb, müsste das Format und der Datenfluss dieser Logs an die Vorgaben der produktiven Infrastruktur angepasst werden.


\subsection{Inhaltliche Kontrolle der publizierten Daten}\label{kap:inhaltlich}
Um dem Datenlieferanten, dem Hersteller der 3D Daten, zu ermöglichen, seine Lieferung inhaltlich zu prüfen, wurde ein sogenanntes codepen.io Projekt angelegt, das auf den S3 Bucket des Prototypen zugreift. Das Resultat, des Belastungstests (Kap. \ref{kap:belastungstest}), kann vorläufig\footnote{Bis am 1. November 2020} begutachtet werden. Es ist sozusagen die Frucht der automatischen Geodatenverarbeitung auf Budget Instanzen und zeigt alle erfassten 3D Gebäude der Schweiz:
\href{https://codepen.io/rebert/pen/ExKZmmE}{codepen.io/rebert/pen/ExKZmmE}. Nur die 3D Gebäude kommen vom S3 Testbucket des Prototypen. Das Terrain und die Vegetation werden vom Geoportal des Bundes bezogen (Abb. \ref{fig:ist_zustand}, Nr. 7).

\subsection{Testen}
Da es sich um eine Datenverarbeitung und nicht um einen öffentlich zugänglichen Web Service handelt, wurde auf End-to-end Testing und Unit-Tests verzichtet. Es wurden lediglich überprüft, ob die Datenverarbeitung auf den Spot Instanzen korrekt durchgeführt werden konnte. Hierzu wurden zwei Arten von Tests gemacht:

\subsubsection{Unerwartete Interrupts}
Da es in der ganzen Entwicklungsphase zu keinem einzigen Interrupt einer Spot Instanz gekommen ist, musste die Stabilität der Datenverarbeitung auf unerwartete Interrupts manuell getestet werden. Um die Testphasen zu verkürzen, wurde die Menge der Eingangsdaten beschränkt\footnote{Auf ein schweizerisches Kartenblatt 1:25'000.}.

Dieser Test wurde iterativ umgesetzt und der Code wurde bei Fehlern fortlaufend angepasst. 


\subsubsection{Belastungstest}\label{kap:belastungstest}
Der komplette Rohdatensatz, alle (erfassten) Gebäude der Schweiz, wurde verarbeitet. Dieser Test war sozusagen die Probe aufs Exempel. Bezüglich Anforderungen an die Rechnerleistung konnte auf Erfahrungswerte aufgebaut werden. Die Anforderungen an die Instanz und dem gemounteten SSD Volumen haben gereicht. Dieser Test wurde auch dafür genutzt, den Output des Prototypen inhaltlich begutachten zu können, wie im vorhergehenden Kapitel (\ref{kap:inhaltlich}) erwähnt. Aber auch die Erkenntnisse zur Wirtschaftlichkeit, Kapitel \ref{kap:wirtschaftlichkeit}, wurden aus diesem Test abgeleitet.

